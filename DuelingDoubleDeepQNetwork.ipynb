{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Environment.ipynb\n",
      "importing Jupyter notebook from HelpFunctions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import PrometheusMetricsCSV as prom\n",
    "from tensorflow.keras import initializers\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import import_ipynb\n",
    "#import SwarmEnvironment as envi\n",
    "import Environment as envi\n",
    "import HelpFunctions as hf\n",
    "import matplotlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QN(keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(D3QN, self).__init__()\n",
    "        self.norm = layers.LayerNormalization(axis=1)\n",
    "        self.layer1 = layers.Dense(64, activation=\"relu\",kernel_initializer='glorot_uniform')\n",
    "        self.layer2 = layers.Dense(128, activation=\"relu\",kernel_initializer='glorot_uniform')\n",
    "        self.layer3 = layers.Dense(256, activation=\"relu\",kernel_initializer='glorot_uniform')\n",
    "\n",
    "        self.Value =  layers.Dense(1, activation=\"linear\")\n",
    "        self.Advantage = keras.layers.Dense(3, activation=\"linear\")\n",
    "    \n",
    "    def call(self, state):\n",
    "        x = self.norm(state)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        V = self.Value(x)\n",
    "        A = self.Advantage(x)\n",
    "        \n",
    "        Q = (V + (A - tf.math.reduce_mean(A, axis=1, keepdims=True)))\n",
    "        \n",
    "        return Q\n",
    "    \n",
    "    def advantage(self, state):\n",
    "        x = self.norm(state)\n",
    "        x = self.layer1(state)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        A = self.Advantage(x)\n",
    "        \n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,max_memory_length):\n",
    "        self.max_memory_length = max_memory_length\n",
    "        \n",
    "        self.action_history = []\n",
    "        self.state_history = []\n",
    "        self.state_next_history = []\n",
    "        self.rewards_history = []\n",
    "        self.done_history = []\n",
    "        \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        self.rewards_history.append(reward)\n",
    "        self.state_next_history.append(next_state)\n",
    "        self.done_history.append(done)\n",
    "        \n",
    "        if len(self.rewards_history) > self.max_memory_length:\n",
    "            del self.state_history[:1]\n",
    "            del self.action_history[:1]\n",
    "            del self.rewards_history[:1]\n",
    "            del self.state_next_history[:1]\n",
    "            del self.done_history[:1]\n",
    "            \n",
    "    def sample_buffer(self, batch_size):\n",
    "        indices = np.random.choice(range(len(self.done_history)), size=batch_size)\n",
    "        states = np.array([self.state_history[i] for i in indices])\n",
    "        actions = [self.action_history[i] for i in indices]\n",
    "        rewards = [self.rewards_history[i] for i in indices]\n",
    "        next_states = np.array([self.state_next_history[i] for i in indices])\n",
    "        dones = tf.convert_to_tensor([float(self.done_history[i]) for i in indices])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, lr, gamma, epsilon, batch_size,\n",
    "                 epsilon_min,epsilon_max, epsilon_random_steps,Number_Of_Episodes,max_steps_per_episode,\n",
    "                 update_target_network,mem_size=42,num_actions=3):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_random_steps = epsilon_random_steps\n",
    "        self.update_target_network = update_target_network\n",
    "        self.epsilon_interval = (self.epsilon_max - self.epsilon_min)\n",
    "        self.Number_Of_Episodes = Number_Of_Episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.mem_size=mem_size\n",
    "        self.num_actions=num_actions\n",
    "        \n",
    "        self.memory = ReplayBuffer(self.mem_size)\n",
    "        \n",
    "        self.model = D3QN()\n",
    "        \n",
    "        self.model_target = D3QN()\n",
    "\n",
    "        self.optimizer = keras.optimizers.RMSprop(learning_rate=self.lr,clipnorm=1.0)\n",
    "        self.loss_function = keras.losses.Huber()\n",
    "        \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "        \n",
    "    def choose_action(self, step_count, episode, state):\n",
    "        if (step_count <= self.epsilon_random_steps and (episode == 0 or episode==1)) or self.epsilon > np.random.rand(1)[0]:\n",
    "            print(f\"Taking random action:{step_count}!\")\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            print(f\"Action based on the model:{step_count}!\")\n",
    "            # CARE HERE, I CHOOSE ACTION BASED ON THE ADVANTAGE NETWORK\n",
    "            action_probs = self.model.advantage(state)\n",
    "            print(action_probs)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "        \n",
    "        self.epsilon -= self.epsilon_interval / (self.Number_Of_Episodes*self.max_steps_per_episode/2)\n",
    "        self.epsilon = max(self.epsilon,self.epsilon_min)\n",
    "        if (episode==self.Number_Of_Episodes-1):\n",
    "            self.epsilon=0\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self,step_count):\n",
    "        if len(self.memory.done_history) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        if  step_count % self.update_target_network == 0:\n",
    "             self.model_target.set_weights(self.model.get_weights())\n",
    "        \n",
    "        state_sample, action_sample, rewards_sample, state_next_sample, done_sample = self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        #DDQN\n",
    "        actions=tf.argmax(self.model.predict(state_next_sample),axis=1)\n",
    "        target_r=self.model_target(state_next_sample)\n",
    "        mask_target = tf.one_hot(actions,self.num_actions)\n",
    "        q_values_next_state=tf.reduce_sum(tf.multiply(target_r,mask_target),axis=1)\n",
    "        updated_q_values = rewards_sample + self.gamma * q_values_next_state\n",
    "        #If final timestep of the episode\n",
    "        updated_q_values = updated_q_values * (1 - done_sample) + done_sample * rewards_sample\n",
    "        \n",
    "        masks = tf.one_hot(action_sample, self.num_actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(state_sample)\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            loss = self.loss_function(updated_q_values, q_action)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Taking random action:1!\n",
      "We are scaling up!\n",
      "im here:414.6633333333333,445.5633333333332,186.74222222222215\n",
      "time:39.823575496673584,reward:-0.635\n",
      "sleeping for:1\n",
      "------------------------------------\n",
      "Taking random action:2!\n",
      "we are scaling down!\n",
      "im here:671.3199999999998,667.4466666666666,346.2555555555555\n",
      "time:32.82739973068237,reward:-1.335\n",
      "sleeping for:8\n",
      "------------------------------------\n",
      "Taking random action:3!\n",
      "We are scaling up!\n",
      "im here:562.8599999999999,636.9766666666667,299.94555555555553\n",
      "time:34.864434003829956,reward:-2.11\n",
      "sleeping for:6\n",
      "------------------------------------\n",
      "Taking random action:4!\n",
      "we are scaling down!\n",
      "im here:693.1266666666666,687.02,360.0488888888889\n",
      "time:34.29184365272522,reward:-2.8099999999999996\n",
      "sleeping for:6\n",
      "------------------------------------\n",
      "Taking random action:5!\n",
      "No scaling action was taken!\n",
      "im here:723.4799999999998,722.3266666666667,381.9355555555555\n",
      "time:20.07270860671997,reward:-3.51\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:6!\n",
      "No scaling down. The minimun number of brokers must be 3!\n",
      "im here:720.1533333333333,718.08,379.4111111111111\n",
      "time:20.06663727760315,reward:-4.21\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:7!\n",
      "We are scaling up!\n",
      "im here:595.3366666666666,745.8233333333333,347.0533333333333\n",
      "time:34.26060128211975,reward:-4.985\n",
      "sleeping for:6\n",
      "------------------------------------\n",
      "Taking random action:8!\n",
      "We are scaling up!\n",
      "im here:487.3066666666666,523.04,236.78222222222217\n",
      "time:35.3741614818573,reward:-5.835\n",
      "sleeping for:5\n",
      "------------------------------------\n",
      "Taking random action:9!\n",
      "No scaling action was taken!\n",
      "im here:458.4933333333332,458.6133333333333,205.7022222222222\n",
      "time:20.075199127197266,reward:-6.685\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:10!\n",
      "No scaling action was taken!\n",
      "im here:458.2533333333333,457.0933333333333,205.11555555555555\n",
      "WARNING:tensorflow:Layer d3qn_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer d3qn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "time:20.60167908668518,reward:-7.534999999999999\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:11!\n",
      "No scaling action was taken!\n",
      "im here:459.33333333333326,458.33333333333326,205.88888888888883\n",
      "time:20.10995101928711,reward:-8.385\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:12!\n",
      "We are scaling up!\n",
      "im here:385.26666666666665,426.86666666666673,170.71111111111114\n",
      "time:36.67106342315674,reward:-9.17\n",
      "sleeping for:4\n",
      "------------------------------------\n",
      "Taking random action:13!\n",
      "We are scaling up!\n",
      "im here:358.0933333333333,389.4733333333333,149.18888888888887\n",
      "time:35.92066764831543,reward:-9.89\n",
      "sleeping for:5\n",
      "------------------------------------\n",
      "Taking random action:14!\n",
      "No scaling up. We have reached the maximum number of brokers!\n",
      "im here:340.2466666666666,340.56,126.93555555555554\n",
      "time:20.251554012298584,reward:-10.610000000000001\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:15!\n",
      "No scaling action was taken!\n",
      "im here:347.17999999999995,346.90666666666664,131.36222222222221\n",
      "time:20.29789161682129,reward:-11.330000000000002\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:16!\n",
      "we are scaling down!\n",
      "im here:355.02666666666664,354.58,136.53555555555553\n",
      "time:34.4649498462677,reward:-11.975000000000001\n",
      "sleeping for:6\n",
      "------------------------------------\n",
      "Taking random action:17!\n",
      "We are scaling up!\n",
      "im here:353.4733333333333,387.32,146.9311111111111\n",
      "time:35.031747817993164,reward:-12.695000000000002\n",
      "sleeping for:5\n",
      "------------------------------------\n",
      "Taking random action:18!\n",
      "we are scaling down!\n",
      "im here:353.3666666666666,352.8066666666666,135.39111111111106\n",
      "time:33.66511940956116,reward:-13.340000000000002\n",
      "sleeping for:7\n",
      "------------------------------------\n",
      "Taking random action:19!\n",
      "We are scaling up!\n",
      "im here:360.0533333333333,384.0933333333333,148.04888888888885\n",
      "time:34.719969511032104,reward:-14.060000000000002\n",
      "sleeping for:6\n",
      "------------------------------------\n",
      "Action based on the model:20!\n",
      "WARNING:tensorflow:Layer layer_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "tf.Tensor([[-416.51706 1754.4269  4361.301  ]], shape=(1, 3), dtype=float32)\n",
      "No scaling action was taken!\n",
      "im here:325.26666666666665,325.9600000000001,117.07555555555555\n",
      "time:20.12160348892212,reward:-14.780000000000003\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:21!\n",
      "No scaling up. We have reached the maximum number of brokers!\n",
      "im here:323.38,324.1933333333333,115.85777777777776\n",
      "time:20.114430904388428,reward:-15.500000000000004\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:22!\n",
      "No scaling up. We have reached the maximum number of brokers!\n",
      "im here:321.88666666666666,323.34666666666664,115.07777777777778\n",
      "time:20.116034746170044,reward:-16.220000000000002\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:23!\n",
      "No scaling up. We have reached the maximum number of brokers!\n",
      "im here:325.11333333333334,324.4133333333333,116.50888888888889\n",
      "time:20.12905263900757,reward:-16.94\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:24!\n",
      "No scaling up. We have reached the maximum number of brokers!\n",
      "im here:290.82,296.9133333333333,95.91111111111111\n",
      "time:20.10252094268799,reward:-17.52\n",
      "sleeping for:20\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # for plotting\n",
    "    delay_consumer = []\n",
    "    delay_producer = []\n",
    "    number_of_brokers = []\n",
    "    time_list = []\n",
    "    episode_reward_history = []\n",
    "\n",
    "    Episodes = 10\n",
    "    max_steps_per_episode = 42\n",
    "    env = envi.Environment(4)\n",
    "    time.sleep(60)\n",
    "    agent = Agent(lr=0.00025,gamma=0.99,epsilon=1.0,epsilon_min=0.1,epsilon_max=1.0,batch_size=10, \\\n",
    "                 epsilon_random_steps = 10,Number_Of_Episodes=Episodes,max_steps_per_episode=42, \\\n",
    "                 update_target_network = 5)\n",
    "    \n",
    "    for episode in range(0,Episodes):\n",
    "        if episode != 0:\n",
    "            print(\"RESETING THE ENVIRONMENT!\")\n",
    "            env.reset()\n",
    "            time.sleep(5)\n",
    "            env = envi.Environment(4)\n",
    "            time.sleep(60)\n",
    "            print(\"ALL GOOD!\")\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        state = env.state()\n",
    "        \n",
    "        for timestep in range(0, max_steps_per_episode):\n",
    "            print(\"------------------------------------\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            step_count +=1\n",
    "            state_tensor = tf.expand_dims(state, 0)\n",
    "            action = agent.choose_action(step_count,episode,state_tensor)\n",
    "            state_next, reward = env.step(action,2)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            #plotting\n",
    "            delay_producer.append(env.producerLatency)\n",
    "            delay_consumer.append(env.consumerLatency)\n",
    "            number_of_brokers.append(env.activeBrokers)\n",
    "            time_list.append(timestep)\n",
    "            \n",
    "            if step_count == max_steps_per_episode:\n",
    "                done = 1 \n",
    "            else:\n",
    "                done = 0\n",
    "                \n",
    "            agent.store_transition(state,action,reward,state_next,done)\n",
    "            \n",
    "            state = state_next\n",
    "            \n",
    "            agent.learn(step_count)\n",
    "            \n",
    "            print(f\"time:{time.time()-start_time},reward:{episode_reward}\")\n",
    "            a = int(round(time.time()-start_time,2))\n",
    "            if(40-a)>0:\n",
    "                print(f\"sleeping for:{(40-a)}\")\n",
    "                time.sleep(40-a)\n",
    "            if done == 1:\n",
    "                hf.saveDataDQN(episode, time_list, delay_producer, delay_consumer, number_of_brokers)\n",
    "                time_list.clear()\n",
    "                delay_producer.clear()\n",
    "                delay_consumer.clear()\n",
    "                number_of_brokers.clear()\n",
    "                break\n",
    "                \n",
    "        episode_reward_history.append(episode_reward)\n",
    "        agent.model.save_weights(\"D3QNmodel.h5\")\n",
    "        agent.model_target.save_weights(\"targetD3QNmodel.h5\")\n",
    "    with open(\"DQN/D3QNrewards.txt\",\"wb\") as fp:\n",
    "        pickle.dump(episode_reward_history,fp)\n",
    "    print('The training is done for D3QN!')\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
