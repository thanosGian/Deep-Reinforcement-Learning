{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Environment.ipynb\n",
      "importing Jupyter notebook from HelpFunctions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import PrometheusMetricsCSV as prom\n",
    "from tensorflow.keras import initializers\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import import_ipynb\n",
    "#import SwarmEnvironment as envi\n",
    "import Environment as envi\n",
    "import HelpFunctions as hf\n",
    "import matplotlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QN(keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(D3QN, self).__init__()\n",
    "        self.norm = layers.LayerNormalization(axis=1)\n",
    "        self.layer1 = layers.Dense(64, activation=\"relu\",kernel_initializer='glorot_uniform')\n",
    "        self.layer2 = layers.Dense(128, activation=\"relu\",kernel_initializer='glorot_uniform')\n",
    "        self.layer3 = layers.Dense(256, activation=\"relu\",kernel_initializer='glorot_uniform')\n",
    "\n",
    "        self.Value =  layers.Dense(1, activation=\"linear\")\n",
    "        self.Advantage = keras.layers.Dense(3, activation=\"linear\")\n",
    "    \n",
    "    def call(self, state):\n",
    "        x = self.norm(state)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        V = self.Value(x)\n",
    "        A = self.Advantage(x)\n",
    "        \n",
    "        Q = (V + (A - tf.math.reduce_mean(A, axis=1, keepdims=True)))\n",
    "        \n",
    "        return Q\n",
    "    \n",
    "    def advantage(self, state):\n",
    "        x = self.norm(state)\n",
    "        x = self.layer1(state)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        A = self.Advantage(x)\n",
    "        \n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,max_memory_length):\n",
    "        self.max_memory_length = max_memory_length\n",
    "        \n",
    "        self.action_history = []\n",
    "        self.state_history = []\n",
    "        self.state_next_history = []\n",
    "        self.rewards_history = []\n",
    "        self.done_history = []\n",
    "        \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        self.rewards_history.append(reward)\n",
    "        self.state_next_history.append(next_state)\n",
    "        self.done_history.append(done)\n",
    "        \n",
    "        if len(self.rewards_history) > self.max_memory_length:\n",
    "            del self.state_history[:1]\n",
    "            del self.action_history[:1]\n",
    "            del self.rewards_history[:1]\n",
    "            del self.state_next_history[:1]\n",
    "            del self.done_history[:1]\n",
    "            \n",
    "    def sample_buffer(self, batch_size):\n",
    "        indices = np.random.choice(range(len(self.done_history)), size=batch_size)\n",
    "        states = np.array([self.state_history[i] for i in indices])\n",
    "        actions = [self.action_history[i] for i in indices]\n",
    "        rewards = [self.rewards_history[i] for i in indices]\n",
    "        next_states = np.array([self.state_next_history[i] for i in indices])\n",
    "        dones = tf.convert_to_tensor([float(self.done_history[i]) for i in indices])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, lr, gamma, epsilon, batch_size,\n",
    "                 epsilon_min,epsilon_max, epsilon_random_steps,Number_Of_Episodes,max_steps_per_episode,\n",
    "                 update_target_network,mem_size=42,num_actions=3):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_random_steps = epsilon_random_steps\n",
    "        self.update_target_network = update_target_network\n",
    "        self.epsilon_interval = (self.epsilon_max - self.epsilon_min)\n",
    "        self.Number_Of_Episodes = Number_Of_Episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.mem_size=mem_size\n",
    "        self.num_actions=num_actions\n",
    "        \n",
    "        self.memory = ReplayBuffer(self.mem_size)\n",
    "        \n",
    "        self.model = D3QN()\n",
    "        \n",
    "        self.model_target = D3QN()\n",
    "\n",
    "        self.optimizer = keras.optimizers.RMSprop(learning_rate=self.lr,clipnorm=1.0)\n",
    "        self.loss_function = keras.losses.Huber()\n",
    "        \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "        \n",
    "    def choose_action(self, step_count, episode, state):\n",
    "        if (step_count <= self.epsilon_random_steps and (episode == 0 or episode==1)) or self.epsilon > np.random.rand(1)[0]:\n",
    "            print(f\"Taking random action:{step_count}!\")\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            print(f\"Action based on the model:{step_count}!\")\n",
    "            # CARE HERE, I CHOOSE ACTION BASED ON THE ADVANTAGE NETWORK\n",
    "            action_probs = self.model.advantage(state)\n",
    "            print(action_probs)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "        \n",
    "        self.epsilon -= self.epsilon_interval / (self.Number_Of_Episodes*self.max_steps_per_episode/2)\n",
    "        self.epsilon = max(self.epsilon,self.epsilon_min)\n",
    "        if (episode==self.Number_Of_Episodes-1):\n",
    "            self.epsilon=0\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self,step_count):\n",
    "        if len(self.memory.done_history) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        if  step_count % self.update_target_network == 0:\n",
    "             self.model_target.set_weights(self.model.get_weights())\n",
    "        \n",
    "        state_sample, action_sample, rewards_sample, state_next_sample, done_sample = self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        #DDQN\n",
    "        actions=tf.argmax(self.model.predict(state_next_sample),axis=1)\n",
    "        target_r=self.model_target(state_next_sample)\n",
    "        mask_target = tf.one_hot(actions,self.num_actions)\n",
    "        q_values_next_state=tf.reduce_sum(tf.multiply(target_r,mask_target),axis=1)\n",
    "        updated_q_values = rewards_sample + self.gamma * q_values_next_state\n",
    "        #If final timestep of the episode\n",
    "        updated_q_values = updated_q_values * (1 - done_sample) + done_sample * rewards_sample\n",
    "        \n",
    "        masks = tf.one_hot(action_sample, self.num_actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(state_sample)\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            loss = self.loss_function(updated_q_values, q_action)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Taking random action:1!\n",
      "We are scaling up!\n",
      "im here:414.6633333333333,445.5633333333332,186.74222222222215\n",
      "time:39.823575496673584,reward:-0.635\n",
      "sleeping for:1\n",
      "------------------------------------\n",
      "Taking random action:2!\n",
      "we are scaling down!\n",
      "im here:671.3199999999998,667.4466666666666,346.2555555555555\n",
      "time:32.82739973068237,reward:-1.335\n",
      "sleeping for:8\n",
      "------------------------------------\n",
      "Taking random action:3!\n",
      "We are scaling up!\n",
      "im here:562.8599999999999,636.9766666666667,299.94555555555553\n",
      "time:34.864434003829956,reward:-2.11\n",
      "sleeping for:6\n",
      "------------------------------------\n",
      "Taking random action:4!\n",
      "we are scaling down!\n",
      "im here:693.1266666666666,687.02,360.0488888888889\n",
      "time:34.29184365272522,reward:-2.8099999999999996\n",
      "sleeping for:6\n",
      "------------------------------------\n",
      "Taking random action:5!\n",
      "No scaling action was taken!\n",
      "im here:723.4799999999998,722.3266666666667,381.9355555555555\n",
      "time:20.07270860671997,reward:-3.51\n",
      "sleeping for:20\n",
      "------------------------------------\n",
      "Taking random action:6!\n",
      "No scaling down. The minimun number of brokers must be 3!\n",
      "im here:720.1533333333333,718.08,379.4111111111111\n",
      "time:20.06663727760315,reward:-4.21\n",
      "sleeping for:20\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # for plotting\n",
    "    delay_consumer = []\n",
    "    delay_producer = []\n",
    "    number_of_brokers = []\n",
    "    time_list = []\n",
    "    episode_reward_history = []\n",
    "\n",
    "    Episodes = 10\n",
    "    max_steps_per_episode = 42\n",
    "    env = envi.Environment(4)\n",
    "    time.sleep(60)\n",
    "    agent = Agent(lr=0.00025,gamma=0.99,epsilon=1.0,epsilon_min=0.1,epsilon_max=1.0,batch_size=10, \\\n",
    "                 epsilon_random_steps = 10,Number_Of_Episodes=Episodes,max_steps_per_episode=42, \\\n",
    "                 update_target_network = 5)\n",
    "    \n",
    "    for episode in range(0,Episodes):\n",
    "        if episode != 0:\n",
    "            print(\"RESETING THE ENVIRONMENT!\")\n",
    "            env.reset()\n",
    "            time.sleep(5)\n",
    "            env = envi.Environment(4)\n",
    "            time.sleep(60)\n",
    "            print(\"ALL GOOD!\")\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        state = env.state()\n",
    "        \n",
    "        for timestep in range(0, max_steps_per_episode):\n",
    "            print(\"------------------------------------\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            step_count +=1\n",
    "            state_tensor = tf.expand_dims(state, 0)\n",
    "            action = agent.choose_action(step_count,episode,state_tensor)\n",
    "            state_next, reward = env.step(action,2)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            #plotting\n",
    "            delay_producer.append(env.producerLatency)\n",
    "            delay_consumer.append(env.consumerLatency)\n",
    "            number_of_brokers.append(env.activeBrokers)\n",
    "            time_list.append(timestep)\n",
    "            \n",
    "            if step_count == max_steps_per_episode:\n",
    "                done = 1 \n",
    "            else:\n",
    "                done = 0\n",
    "                \n",
    "            agent.store_transition(state,action,reward,state_next,done)\n",
    "            \n",
    "            state = state_next\n",
    "            \n",
    "            agent.learn(step_count)\n",
    "            \n",
    "            print(f\"time:{time.time()-start_time},reward:{episode_reward}\")\n",
    "            a = int(round(time.time()-start_time,2))\n",
    "            if(40-a)>0:\n",
    "                print(f\"sleeping for:{(40-a)}\")\n",
    "                time.sleep(40-a)\n",
    "            if done == 1:\n",
    "                hf.saveDataDQN(episode, time_list, delay_producer, delay_consumer, number_of_brokers)\n",
    "                time_list.clear()\n",
    "                delay_producer.clear()\n",
    "                delay_consumer.clear()\n",
    "                number_of_brokers.clear()\n",
    "                break\n",
    "                \n",
    "        episode_reward_history.append(episode_reward)\n",
    "        agent.model.save_weights(\"D3QNmodel.h5\")\n",
    "        agent.model_target.save_weights(\"targetD3QNmodel.h5\")\n",
    "    with open(\"DQN/D3QNrewards.txt\",\"wb\") as fp:\n",
    "        pickle.dump(episode_reward_history,fp)\n",
    "    print('The training is done for D3QN!')\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
