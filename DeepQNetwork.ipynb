{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import PrometheusMetricsCSV as prom\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import import_ipynb\n",
    "# import SwarmEnvironment as envi\n",
    "import Environment as envi\n",
    "import HelpFunctions as hf\n",
    "import matplotlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")\n",
    "batch_size = 10\n",
    "max_steps_per_episode = 42 # scaling action every 40s\n",
    "\n",
    "env = envi.Environment(4)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 3\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    \n",
    "    inputs = layers.Input(shape=(37,))\n",
    "    norm = layers.LayerNormalization(axis=1)(inputs)\n",
    "    layer1 = layers.Dense(64, activation=\"relu\",kernel_initializer='glorot_uniform')(norm)\n",
    "    layer2 = layers.Dense(128, activation=\"relu\",kernel_initializer='glorot_uniform')(layer1)\n",
    "    layer3 = layers.Dense(256, activation=\"relu\",kernel_initializer='glorot_uniform')(layer2)\n",
    "    \n",
    "    action = layers.Dense(num_actions, activation=\"linear\",kernel_initializer='glorot_uniform')(layer3)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "\n",
    "model = create_q_model()\n",
    "\n",
    "model_target = create_q_model()\n",
    "\n",
    "# model.load_weights(\"model.h5\")\n",
    "# model_target.load_weights(\"target_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section for ploting\n",
    "delay_consumer = []\n",
    "delay_producer = []\n",
    "number_of_brokers = []\n",
    "time_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.00025,clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "Number_Of_Episodes = 10\n",
    "\n",
    "# Number of random actions to take and observe output\n",
    "epsilon_random_steps = 10 # edw eixame 10\n",
    "max_memory_length = 42 # the same as the number of timesteps per episode\n",
    "\n",
    "# Train the model after 1 action\n",
    "update_after_actions = 1\n",
    "# How often to update the target network\n",
    "update_target_network = 5\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "for episode in range(0,Number_Of_Episodes):\n",
    "    if episode != 0:\n",
    "        print(\"RESETING THE ENVIRONMENT!\")\n",
    "        env.reset()\n",
    "        time.sleep(5)\n",
    "        env = envi.Environment(4)\n",
    "        time.sleep(60)\n",
    "        print(\"ALL GOOD!\")\n",
    "        \n",
    "    episode_reward = 0\n",
    "    step_count = 0\n",
    "    state = env.state()\n",
    "    \n",
    "    for timestep in range(0, max_steps_per_episode):\n",
    "        print(\"------------------------------------\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        step_count +=1\n",
    "        if (step_count < epsilon_random_steps and (episode == 0 or episode==1)) or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            print(f\"Taking random action:{step_count}!\")\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            print(f\"Action based on the model:{step_count}!\")\n",
    "            state_tensor = tf.expand_dims(state, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / (Number_Of_Episodes*max_steps_per_episode/2)\n",
    "        epsilon = max(epsilon,epsilon_min)\n",
    "        # In the last 2 episodes only exploit what you have learned\n",
    "        if (episode==Number_Of_Episodes-1) or (episode==Number_Of_Episodes-2):\n",
    "            epsilon=0\n",
    "\n",
    "        # Apply the sampled action in our environment, \n",
    "        # second parameter at step function denotes the appropriate reward function\n",
    "        state_next, reward =  env.step(action,2)\n",
    "        print(reward)\n",
    "        episode_reward += reward\n",
    "        #plotting\n",
    "        delay_producer.append(env.producerLatency)\n",
    "        delay_consumer.append(env.consumerLatency)\n",
    "        number_of_brokers.append(env.activeBrokers)\n",
    "        time_list.append(timestep)\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        rewards_history.append(reward)\n",
    "        if step_count == max_steps_per_episode:\n",
    "            done_history.append(1) # True\n",
    "            done = 1 \n",
    "        else:\n",
    "            done_history.append(0) # False\n",
    "            done = 0\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "        # Update once batch size is over 32\n",
    "        if len(done_history) > batch_size: #edw prepei na paei batch_size\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "\n",
    "            # If final timestep of the episode\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) + done_sample * rewards_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(state_sample)\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # update the target network\n",
    "        if step_count % update_target_network == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            template = \"running reward: {:.2f} at step {}\"\n",
    "            print(template.format(episode_reward,step_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"time:{time.time()-start_time},reward:{episode_reward}\")\n",
    "        a = int(round(time.time()-start_time,2))\n",
    "        if(40-a)>0:\n",
    "            print(f\"sleeping for:{(40-a)}\")\n",
    "            time.sleep(40-a)\n",
    "        if done == 1:\n",
    "            hf.saveDataDQN(episode, time_list, delay_producer, delay_consumer, number_of_brokers)\n",
    "            time_list.clear()\n",
    "            delay_producer.clear()\n",
    "            delay_consumer.clear()\n",
    "            number_of_brokers.clear()\n",
    "            \n",
    "            break\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    model.save_weights(\"DQNmodel.h5\")\n",
    "    model_target.save_weights(\"targetDQNmodel.h5\")\n",
    "with open(\"DQN/DQNrewards.txt\",\"wb\") as fp:\n",
    "    pickle.dump(episode_reward_history,fp)\n",
    "print('The training is done for DQN!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
